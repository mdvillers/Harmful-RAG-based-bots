# Harmful-RAG-based-bots
Harmful RAG Based Bots: Uncovering The Impact of Public Sources

This project investigates how RAG-based AI assistants can be manipulated through adversarial content injected into public, freely commentable sources. We simulate realistic adversarial user comments using automated methods such as AutoDAN to analyze how poisoned content influences retrieval pipelines and leads to potentially unsafe or harmful LLM responses.

Our experiments span multiple retrievers and large language models under varying poisoning budgets. By evaluating how different systems respond to targeted injections, we aim to reveal the mechanisms that make RAG systems vulnerable and quantify the impact on both retrieval quality and downstream generation.

The findings emphasize the need for strong content-filtering pipelines and poisoning-aware retrieval strategies. Ultimately, this work contributes to building safer, more reliable, and more trustworthy RAG-based AI assistants.
